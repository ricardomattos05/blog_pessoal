---
title: "Desafio Kaggle - ML Curso-R"
author: "Ricardo Mattos"
date: "12/07/2020"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
library(readr)
library(tidymodels)
library(ggplot2)
library(skimr)
library(RCurl)
library(kableExtra)
library(gridExtra)
library(glue)
library(forcats)
```


# Leitura da base

## Informações preliminares

```{r}
adult <- read_rds("adult.rds")


skim(adult)

```

<br> As variáveis parecem estar com formatos corretos. Ponto de atenção para as variáveis `wokclass`, `occupation` e `native_country`, que apresentam valores missing.  </br>

## Amostragem

Separando os dados em treino e teste:

```{r}
set.seed(32)

adult_split <- initial_split(adult, prop = 0.8, strata = resposta)

adult_train <- training(adult_split)
adult_test <- testing(adult_split)

```


## AED
```{r, message=FALSE, warning=FALSE}

GGally::ggpairs(adult %>% select(all_numeric(),resposta))
GGally::ggpairs(adult %>% select(all_nominal(),resposta,-id), cardinality_threshold = 41)

# adult %>% filter(is.na(occupation)|is.na(workclass) ) %>% count(occupation,workclass)
# adult %>% filter(is.na(workclass)) %>% count(resposta)   
# 
# 
# adult %>% 
#   # filter(is.na(workclass)) %>%
#   # mutate("workclass_na" = if_else(is.na(workclass), "1","0")) %>% #Variável com NA para analise de casos NA
#   # select(-workclass) %>% 
#   select_if(is.character) %>% #apenas categóricas
#   gather(key=group, value=value, -resposta) %>% #pivot
#   ggplot(aes(y = value, fill= resposta)) +
#   geom_histogram(stat="count") +
#   facet_wrap(~ group, scales = "free")
#   
#   
# DataExplorer::create_report(adult)
# 
#  
#   ggplot(adult, aes(sample = log(fnlwgt) )) +
#   stat_qq() +
#   stat_qq_line()
#   
#   ggplot(adult,  aes(log(fnlwgt), fill = resposta ) )+
#     geom_density(alpha = 0.6)
# 
#   rm(AED_biv)    
devtools::source_url("https://raw.githubusercontent.com/ricardomattos05/functions/master/function_AED_bivariada.R")
# 
# # Response_var <- glue("resposta")
# 
adult2 <- adult %>%
            mutate(resposta = if_else(resposta == ">50K", 1, 0))
# 
# 
AED_biv(adult2,glue("resposta"),"Pre")



```

# Modelagem

## Data Prep

Os tratamentos necessários e observados na AED, que foi feita utilizando o pacote `DataExplorer` e a função [`AED_biv`](https://github.com/ricardomattos05/functions/blob/master/function_AED_bivariada.R) que gerei para entender o comportamento das variáveis com relação a variável resposta, serão armazenados utilizando o recipes para ser utilizado tanto para treinar os modelos como para testar posteriormente.


```{r}

adult_recipe <- 
  recipe(resposta ~ ., data = adult_train) %>% 
  step_mutate(
    occupation = case_when(
      is.na(occupation) ~ "Desempregado",
      TRUE ~ as.character(occupation)),
    workclass = case_when(
      is.na(workclass) ~ "Desempregado",
      TRUE ~ as.character(workclass)),
    native_country = case_when(
      native_country == "United-States" ~ "USA",
      TRUE ~ "other"),
    capital_total = capital_gain - capital_loss
    , #or +
    marital_status = case_when(
      marital_status == "Married-AF-spouse" | marital_status == "Married-civ-spouse" ~ "Married",
      TRUE ~ as.character(marital_status))
    ,
    education = case_when(education = c("Preschool", "1st-4th", "5th-6th", "7th-8th", "9th", "10th", "11th", "12th") ~"HS-not-grad",
                          TRUE ~ as.character(education))
                          
    # education = fct_collapse(
    #   education,
    #   "HS-not-grad" = c("Preschool", "1st-4th", "5th-6th", "7th-8th", "9th", "10th", "11th", "12th"),
    #   Associates = c("Assoc-acdm", "Assoc-voc")
    # ),
    # education = fct_relevel(education, "HS-not-grad", "HS-grad", "Some-college", "Associates", "Bachelors", "Masters", "Prof-school", "Doctorate")
  ) %>% 
  step_rm(id, capital_gain, capital_loss)%>% 
  step_string2factor(all_nominal()) %>%
  # step_log(fnlwgt, age) %>%  
  step_normalize(all_numeric()) %>% 
  step_zv(all_predictors()) %>%
  # step_center(all_numeric()) %>%
  # step_scale(all_numeric()) %>%
  step_novel(all_nominal(), -all_outcomes()) %>% 
  step_dummy(all_nominal(), -all_outcomes())

# bake(prep(adult_recipe), adult_train)

adult_wf <- 
  workflow() %>% 
  add_recipe(adult_recipe)




```


## Cross-Validation

Especificando a validação cruzada:

```{r}
set.seed(32)
adult_vfold <- vfold_cv(adult_train, v = 5, strata = resposta)
adult_vfold
```

## Modelos {.tabset}

Os modelos que serão ajustados:

  * Decision tree
  * Random Forest
  * xgboost

### Decision tree

Especificando modelo:

```{r}
adult_tree <- 
  decision_tree(
    min_n = tune(),
    cost_complexity = tune(), 
    tree_depth = tune()) %>%
  set_mode("classification") %>%
  set_engine("rpart")

adult_tree
```
Workflow para decision tree:

```{r}

workflow_adult_tree <- 
  adult_wf %>% 
  add_model(adult_tree)


```

Parâmentros:

```{r}
hiperparams <- parameters(
 adult_tree
)
hiperparams
```

Grid:

```{r}
set.seed(32)
tree_grid <- grid_max_entropy(hiperparams, size = 10)

```


Efetuando tunagem de hiperparâmetros:

```{r}

tree_tune <- 
  workflow_adult_tree %>% 
  tune_grid(
    resamples = adult_vfold,
    grid = tree_grid,
    control = control_grid(save_pred = TRUE, verbose = T, allow_par = F),
    metrics = metric_set(roc_auc)
  )

```

```{r}
autoplot(tree_tune)
show_best(tree_tune, "roc_auc")

tree_best_hiperparams <- select_best(tree_tune) #cp = 1.069415e-09 td =	8	 min_n = 19	 Model04
tree_best_hiperparams

```

Finalizando WF:

```{r}
workflow_tree_final <- finalize_workflow(
  workflow_adult_tree,
  tree_best_hiperparams
)

workflow_tree_final
```

Verificando importância dos atributos:

```{r}
workflow_tree_final %>%
  fit(adult_train) %>%
  pull_workflow_fit() %>%
  vip::vip(geom = "col")
```

Modelo final:


```{r}

tree_final <- last_fit(workflow_tree_final, adult_split)
collect_metrics(tree_final)

```


### Random Forest

Especificando modelo:

```{r}
adult_rf <- 
  rand_forest(
    min_n = tune(),
    mtry = tune(),
    trees = tune()) %>%
  set_mode("classification") %>%
  set_engine("randomForest")

adult_rf
```

Workflow para random forest:

```{r}

workflow_adult_rf <- 
  adult_wf %>% 
  add_model(adult_rf)


```

Parâmentros:

```{r}
rf_hiperparams <- parameters(
  trees(),
  min_n(),
  finalize(mtry(), adult_train)
  
)
rf_hiperparams
```
Grid:

```{r}
set.seed(32)

#grade regular
# rf_grid <- grid_regular( 
#   mtry(range = c(10, 30)),
#   min_n(range = c(2, 8))
#   , levels = 10)

#grid_max_entropy
rf_grid2 <- grid_max_entropy(
      rf_hiperparams, size = 10)


```

Efetuando tunagem de hiperparâmetros:

```{r}
#doParallel::registerDoParallel()

# grade regular
# seed(123)
# rf_tune <- 
#   workflow_adult_rf %>% 
#   tune_grid(
#     resamples = adult_vfold,
#     grid = rf_grid,
#     control = control_grid(save_pred = TRUE, verbose = T, allow_par = F),
#     metrics = metric_set(roc_auc)
#   )#mtry = 14	min_n = 8	  Model63


## grade cubo latino
# seed(123)
# rf_tune2 <- 
#   workflow_adult_rf %>% 
#   tune_grid(
#     resamples = adult_vfold,
#     grid = 10,
#     control = control_grid(save_pred = TRUE, verbose = T, allow_par = F),
#     metrics = metric_set(roc_auc)
#   )

## Grade Max_Entropy
seed(123)
rf_tune3 <- 
  workflow_adult_rf %>% 
  tune_grid(
    resamples = adult_vfold,
    grid = rf_grid2,
    control = control_grid(save_pred = TRUE, verbose = T, allow_par = F),
    metrics = metric_set(roc_auc)
  )
#doParallel::stopImplicitCluster()
```

```{r}
autoplot(rf_tune3)
show_best(rf_tune3,"roc_auc")

rf_best_hiperparams <- select_best(rf_tune3) 
rf_best_hiperparams #mtry = 14	min_n = 34	  trees = 1731 (roc_auc = 0.9135926)

```

Finalizando WF:

```{r}
workflow_rf_final <- finalize_workflow(
  workflow_adult_rf,
  rf_best_hiperparams
)

workflow_rf_final
```

Verificando importância dos atributos:

```{r}
workflow_rf_final %>%
  fit(adult_train) %>%
  pull_workflow_fit() %>%
  vip::vip(geom = "col")
```

Modelo final:

```{r}

rf_final <- last_fit(workflow_rf_final, adult_split)
collect_metrics(rf_final)

```

### Xgboost1

Especificando modelo:

```{r}
adult_xgb <- 
  boost_tree(
   mtry = 5, 
  trees = 1000, 
  min_n = tune(), 
  tree_depth = tune(),
  loss_reduction = tune(), 
  learn_rate = tune(), 
  sample_size = 0.75
  ) %>%
  set_mode("classification") %>%
  set_engine("xgboost")

adult_xgb
```
Workflow para Xgboost:

```{r}

workflow_adult_xgb <- 
  adult_wf %>% 
  add_model(adult_xgb)


```

Parâmentros:

```{r}
# xgb_hiperparams <- parameters(
#   trees(),
#   min_n(),
#   finalize(mtry(), adult_train)
#   
# )
# rf_hiperparams
```

Grid:

```{r}
set.seed(32)

xgb_grid <- parameters(adult_xgb) %>% 
    finalize(adult_train) %>% 
    grid_random(size = 200)

head(xgb_grid)

parameters(adult_xgb3) %>% 

  finalize(mtry(), bake(prep(adult_recipe), adult_train))


```

Efetuando tunagem de hiperparâmetros:

```{r}
# grid search
xgb_tune <-
  workflow_adult_xgb %>%
    tune_grid(
        resamples = adult_vfold,
        grid = xgb_grid,
        control = control_grid(verbose = TRUE),
        metrics = metric_set(roc_auc)
    )

```


```{r}
autoplot(xgb_tune)
show_best(xgb_tune,"roc_auc")

xgb_best_hiperparams <- select_best(xgb_tune) 
xgb_best_hiperparams #7	10	0.08023151	2.906732e-10	Model008 (roc_auc = 0.9237762)

```

Finalizando WF:

```{r}
workflow_xgb_final <- finalize_workflow(
  workflow_adult_xgb,
  xgb_best_hiperparams
)

workflow_xgb_final
```

Verificando importância dos atributos:

```{r}
workflow_xgb_final %>%
  fit(adult_train) %>%
  pull_workflow_fit() %>%
  vip::vip(geom = "col")
```

Modelo final:

```{r}

xgb_final <- last_fit(workflow_xgb_final, adult_split)
collect_metrics(xgb_final)

# Original, pré mutates
# accuracy	binary	0.8700660		
# roc_auc	binary	0.9228654	

```


### Xgboost2

Tunando mtry, trees e sample size:

```{r}
adult_xgb2 <- 
  boost_tree(
   mtry = 5, 
  trees = 1000, 
  min_n = 5, 
  tree_depth = 8,
  loss_reduction = 1.881944e-05, 
  learn_rate = 0.07024783, 
  sample_size = 0.75
  ) %>%
  set_mode("classification") %>%
  set_engine("xgboost")

adult_xgb2
```
Workflow para Xgboost:

```{r}

workflow_adult_xgb2 <- 
  adult_wf %>% 
  add_model(adult_xgb2)


```

Grid:

```{r}
# set.seed(32)
# 
# xgb_grid <- parameters(adult_xgb) %>% 
#     finalize(adult_train) %>% 
#     grid_random(size = 200)
# 
# head(xgb_grid)


```

Efetuando tunagem de hiperparâmetros:

```{r}
# grid search
# xgb_tune <-
#   workflow_adult_xgb %>%
#     tune_grid(
#         resamples = adult_vfold,
#         grid = xgb_grid,
#         control = control_grid(verbose = TRUE),
#         metrics = metric_set(roc_auc)
#     )

```


```{r}
# autoplot(xgb_tune)
# show_best(xgb_tune,"roc_auc")
# 
# xgb_best_hiperparams <- select_best(xgb_tune) 
# xgb_best_hiperparams #7	10	0.08023151	2.906732e-10	Model008 (roc_auc = 0.9237762)

```


Finalizando WF:

```{r}
workflow_xgb_final2 <- finalize_workflow(
  workflow_adult_xgb2,
  parameters(workflow_adult_xgb2) 
)

workflow_xgb_final2
```

Verificando importância dos atributos:

```{r}
workflow_xgb_final2 %>%
  fit(adult_train) %>%
  pull_workflow_fit() %>%
  vip::vip(geom = "col")
```

Modelo final:

```{r}

xgb_final2 <- last_fit(workflow_xgb_final2, adult_split)
collect_metrics(xgb_final2)

```


### Xgboost3

Tunando mtry, trees e sample size:

```{r}
adult_xgb3 <- 
  boost_tree(
   mtry = tune(), 
  trees = tune(), 
  min_n = xgb_best_hiperparams$min_n, 
  tree_depth = xgb_best_hiperparams$tree_depth,
  loss_reduction = xgb_best_hiperparams$loss_reduction, 
  learn_rate = xgb_best_hiperparams$learn_rate, 
  sample_size = tune()
  ) %>%
  set_mode("classification") %>%
  set_engine("xgboost")

adult_xgb3
```
Workflow para Xgboost:

```{r}

workflow_adult_xgb3 <- 
  adult_wf %>% 
  add_model(adult_xgb3)


```

Grid:

```{r}
set.seed(32)

xgb_grid <- parameters(adult_xgb3) %>%
    finalize(adult_train) %>%
    grid_random(size = 20)

head(xgb_grid)


```

Efetuando tunagem de hiperparâmetros:

```{r}
# grid search
xgb_tune3 <-
  workflow_adult_xgb3 %>%
    tune_grid(
        resamples = adult_vfold,
        grid = xgb_grid,
        control = control_grid(verbose = TRUE),
        metrics = metric_set(roc_auc)
    )

```


```{r}
autoplot(xgb_tune3)
show_best(xgb_tune3,"roc_auc")

xgb3_best_hiperparams <- select_best(xgb_tune3)
xgb3_best_hiperparams #11	419	0.7933403	Model12 (roc_auc = 0.9244971)

```


Finalizando WF:

```{r}
workflow_xgb_final3 <- finalize_workflow(
  workflow_adult_xgb3,
  xgb3_best_hiperparams 
)

workflow_xgb_final3
```

Verificando importância dos atributos:

```{r}
workflow_xgb_final3 %>%
  fit(adult_train) %>%
  pull_workflow_fit() %>%
  vip::vip(geom = "col")
```

Modelo final:

```{r}

xgb_final3 <- last_fit(workflow_xgb_final3, adult_split)
collect_metrics(xgb_final3)

```

### Xgboost4: Grid latin hypercube 

Tunando mtry, trees e sample size:

```{r}
adult_xgb4 <- 
  boost_tree(
   mtry = tune(), 
  trees = 1000, 
  min_n = tune(), 
  tree_depth = tune(),
  loss_reduction = tune(), 
  learn_rate = tune(), 
  sample_size = tune()
  ) %>%
  set_mode("classification") %>%
  set_engine("xgboost")

adult_xgb4
```
Workflow para Xgboost:

```{r}

workflow_adult_xgb4 <- 
  adult_wf %>% 
  add_model(adult_xgb4)


```

Grid:

```{r}
set.seed(32)

xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), adult_train),
  learn_rate(),
  size = 30
)

head(xgb_grid)


```

Efetuando tunagem de hiperparâmetros:

```{r}
# grid search
xgb_tune4 <-
  workflow_adult_xgb4 %>%
    tune_grid(
        resamples = adult_vfold,
        grid = xgb_grid,
        control = control_grid(verbose = TRUE),
        metrics = metric_set(roc_auc)
    )

```


```{r}
autoplot(xgb_tune4)
show_best(xgb_tune4,"roc_auc")

xgb4_best_hiperparams <- select_best(xgb_tune4)
xgb4_best_hiperparams #15	8	11	0.03820848	0.06904151	0.7616527	Model07 (roc_auc = 0.9227588)


```


Finalizando WF:

```{r}
workflow_xgb_final4 <- finalize_workflow(
  workflow_adult_xgb4,
  xgb4_best_hiperparams 
)

workflow_xgb_final4
```

Verificando importância dos atributos:

```{r}
workflow_xgb_final4 %>%
  fit(adult_train) %>%
  pull_workflow_fit() %>%
  vip::vip(geom = "col")
```

Modelo final:

```{r}

xgb_final4 <- last_fit(workflow_xgb_final4, adult_split)
collect_metrics(xgb_final4)

```
### Xgboost5

Tunando mtry, trees e sample size:

```{r}
adult_xgb5 <- 
  boost_tree(
   mtry = tune(), 
  trees = tune(), 
  min_n = tune(), 
  tree_depth = tune(),
  # loss_reduction = tune(), 
  learn_rate = tune(), 
  # sample_size = tune()
  ) %>%
  set_mode("classification") %>%
  set_engine("xgboost")

adult_xgb5
```

Workflow para Xgboost:

```{r}

workflow_adult_xgb5 <- 
  adult_wf %>% 
  add_model(adult_xgb5)


```

Grid:

```{r}
set.seed(32)

xgb_grid <- parameters(adult_xgb5) %>%
    finalize(bake(prep(adult_recipe),adult_train)) %>%
    grid_max_entropy(size = 20)

xgb_grid



```

Efetuando tunagem de hiperparâmetros:

```{r}
library(doParallel)

# numCores <- parallel::detectCores() #Num de cores para dividir o trab
# 
# doParallel::registerDoParallel(numCores) #Registrando cores e iniciando parallel computing framework
# 
# foreach::getDoParWorkers() #confirmando o numero de cores utilizados

cl <- makePSOCKcluster(parallel::detectCores(logical = FALSE))
registerDoParallel(cl)

getDoParWorkers()

clusterEvalQ(cl, {library(tidymodels)})

library("doFuture")
all_cores <- parallel::detectCores(logical = FALSE) - 1

registerDoFuture()
cl <- makeCluster(all_cores)
plan(future::cluster, workers = cl)

# grid search
ini <- Sys.time()
xgb_tune5 <-
  workflow_adult_xgb5 %>%
    tune_grid(
        resamples = adult_vfold,
        grid = xgb_grid,
        control = control_grid(verbose = TRUE),
        metrics = metric_set(roc_auc)
    )
Sys.time()- ini #Time difference of 1.034167 hours ( 35 mins with parallel)

foreach::registerDoSEQ()
```


```{r}
autoplot(xgb_tune5)
show_best(xgb_tune5,"roc_auc")

xgb5_best_hiperparams <- select_best(xgb_tune5)
xgb5_best_hiperparams #10	1636	6	10	0.006471534	0.001861842	0.9861206	Model04 (roc_auc = 0.9261595)

```


Finalizando WF:

```{r}
workflow_xgb_final5 <- finalize_workflow(
  workflow_adult_xgb5,
  xgb5_best_hiperparams 
)

workflow_xgb_final5
```

Verificando importância dos atributos:

```{r}
workflow_xgb_final5 %>%
  fit(adult_train) %>%
  pull_workflow_fit() %>%
  vip::vip(geom = "col")
```

Modelo final:

```{r}

xgb_final5 <- last_fit(workflow_xgb_final5, adult_split)
collect_metrics(xgb_final5)

```

### Xgboost6

Tunando mtry, trees e sample size:

```{r}
adult_xgb6 <- 
  boost_tree(
    tree_depth = 7, 
    trees = 1347,
    learn_rate = 1.566693e-02,
    mtry = 33,
    min_n = 5
  ) %>%
  set_mode("classification") %>%
  set_engine("xgboost")

adult_xgb6 %>% summary()

```
Workflow para Xgboost:

```{r}

workflow_adult_xgb6 <- 
  adult_wf %>% 
  add_model(adult_xgb6)


```

Grid:

```{r}
# set.seed(32)
# 
# xgb_grid <- parameters(adult_xgb5) %>%
#     finalize(bake(prep(adult_recipe),adult_train)) %>%
#     grid_max_entropy(size = 20)
# 
# xgb_grid



```

Efetuando tunagem de hiperparâmetros:

```{r}

# numCores <- parallel::detectCores() #Num de cores para dividir o trab
# 
# doParallel::registerDoParallel(numCores) #Registrando cores e iniciando parallel computing framework
# 
# foreach::getDoParWorkers() #confirmando o numero de cores utilizados
# 
# # grid search
# ini <- Sys.time()
# xgb_tune5 <-
#   workflow_adult_xgb5 %>%
#     tune_grid(
#         resamples = adult_vfold,
#         grid = xgb_grid,
#         control = control_grid(verbose = TRUE),
#         metrics = metric_set(roc_auc)
#     )
# Sys.time()- ini #Time difference of 1.034167 hours
# 
# foreach::registerDoSEQ()
```


```{r}
# autoplot(xgb_tune5)
# show_best(xgb_tune5,"roc_auc")
# 
# xgb5_best_hiperparams <- select_best(xgb_tune5)
# xgb5_best_hiperparams #10	1636	6	10	0.006471534	0.001861842	0.9861206	Model04 (roc_auc = 0.9261595)

```


Finalizando WF:

```{r}
workflow_xgb_final6 <- finalize_workflow(
  workflow_adult_xgb6,
  parameters(adult_xgb6) 
)
workflow_xgb_final6

```

Verificando importância dos atributos:

```{r}
workflow_xgb_final6 %>%
  fit(adult_train) %>%
  pull_workflow_fit() %>%
  vip::vip(geom = "col")
```

Modelo final:

```{r}

xgb_final6 <- last_fit(workflow_xgb_final6, adult_split)
collect_metrics(xgb_final6)

# Comparação dos Modelos

```


```{r}
bind_rows(
 tree_final %>%
  collect_predictions() %>% 
  mutate(id = "Decision tree")
  ,
 rf_final %>%
  collect_predictions() %>% 
  mutate(id = "Random Forest")
 ,
  xgb_final %>%
  collect_predictions() %>% 
  mutate(id = "xgboost")
  ,
  xgb_final2 %>%
  collect_predictions() %>% 
  mutate(id = "xgboost2")
  ,
  xgb_final3 %>%
  collect_predictions() %>% 
  mutate(id = "xgboost3")
  ,
  xgb_final4 %>%
  collect_predictions() %>% 
  mutate(id = "xgboost4")
  ,
  xgb_final5 %>%
  collect_predictions() %>% 
  mutate(id = "xgboost5")
  ,
  xgb_final6 %>%
  collect_predictions() %>% 
  mutate(id = "xgboost6")
) %>% 
  group_by(id) %>% 
  nest() %>% 
  ungroup() %>% 
  mutate(roc = map(data, ~roc_curve(.x, truth = resposta, `.pred_>50K`)),
         auc = map_dbl(data, ~roc_auc(.x, truth = resposta, `.pred_>50K`) %>% 
                         pull(.estimate) %>% round(4)),
         id = paste0(id, " auc: ", auc)) %>% 
  select(-data) %>% 
  unnest(cols = c(roc)) %>% 
  ggplot() +
  aes(x = 1 - specificity, y = sensitivity, color = id) +
  geom_path() +
  geom_abline(lty = 3) +
  ggtitle("Curva Roc")


```

# Scoragem para submeter resultado

```{r}

adult_val <- readr::read_rds("adult_val.rds")

xgboost_modelo_final <- adult_xgb5 %>% 
    finalize_model(xgb5_best_hiperparams)

adult_fit <- 
  fit(xgboost_modelo_final,
    formula = resposta ~.,  
    data = bake(prep(adult_recipe), new_data = adult))

adult_val$more_than_50k <- 
  predict(adult_fit, 
          bake(prep(adult_recipe), new_data = adult_val),
          type = "prob")$`.pred_>50K`
```

Matriz de confusão:

```{r}
adult_val %>% 
  transmute(resposta = factor(resposta, levels = c(">50K", "<=50K")), 
            more_than_50k = ifelse(more_than_50k > 0.5, ">50K", "<=50K") %>% 
              factor(levels = c(">50K", "<=50K"))) %>% 
  table() %>% 
  caret::confusionMatrix()

library('pROC')
plot(roc(adult_val$resposta, adult_val$more_than_50k))


adult_val %>% 
  roc_auc(truth = as.factor(resposta), more_than_50k)

#xgb with new step_mutate: 0.925582	
#xgb2 with new step_mutate: 0.9262863	
#xgb5: 0.9247659 
#xgb5_2 tunnando apenas 5 params: 0.9277751	
#xgb6:  0.9288229	
```


```{r}
submissao <- adult_val %>% select(id, more_than_50k)
#readr::write_csv(submissao, "submissao_xgb5_2.csv")
```

